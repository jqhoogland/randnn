{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import List, Callable, Optional, Any\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from randnn import ContinuousNN, get_attractor_dimension, scaling_analysis, downsample, downsample_split, avg_over\n",
    "from randnn.utils import np_cache, eigsort, count_cycles\n",
    "from randnn.plotting import *\n",
    "from randnn.systems import DoubleWell\n",
    "from randnn.transfer_operator import *\n",
    "\n",
    "rc('text', usetex=True)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the transfer operator approach\n",
    "\n",
    "#### 1. Measurement / Simulation\n",
    "\n",
    "We begin by simulating the data, a $N$-dimensional time series of $T$ timesteps.\n",
    "\n",
    "$T \\times N$\n",
    "\n",
    "#### 2. Initial dimensional reduction\n",
    "\n",
    "In the case of the random neural network, we know that the chaotic dynamics exist near a much lower-dimensional manifold. Therefore, we don't need to deal with a very high-dimensional space (and thus computationally intractable), but can play with a much lower dimensional space.\n",
    "\n",
    "Using PCA we project down to a dimensionality $D$. We can choose this ourselves, or choose it automatically via some measure of manifold dimensionality. For example, the Lyapunov-spectrum-derived attractor dimension.\n",
    "\n",
    "Note: this choice of $D$ may not be optimal (Engelken et al. 2020 found that the \"PCA dimension\" could vary independently from the attractor dimension).\n",
    "\n",
    "$T \\times D$\n",
    "\n",
    "#### 3. Downsampling \n",
    "\n",
    "We are interested in the long-term behavior of the system (and have finitely powerful computers), so we downsample the trajectory with a downsampling rate $D$ (i.e. keeping one in $D$ timesteps).\n",
    "\n",
    "To determine the optimal number of delays, we choose the number of delays for which the KS entropy stops decresaing.\n",
    "\n",
    "$T/R \\times D$\n",
    "\n",
    "#### 4. (Optional) Delay embedding\n",
    "\n",
    "To increase the Markovianity of our state, we may stack delayed copies of measurements within a short time window.\n",
    "\n",
    "$T/R - K + 1 \\times KD$\n",
    "\n",
    "\n",
    "#### 5. Second dimensionality reduction\n",
    "\n",
    "From the delay-embedded space, we can project down to a more manageable $M$ dimensions.\n",
    "\n",
    "$T/R - K + 1 \\times M$\n",
    "\n",
    "\n",
    "#### Transfer Matrix\n",
    "\n",
    "From any time-series $t \\times d$, we can readily compute a transfer matrix.\n",
    "\n",
    "First, we cluster the time series into $C$ clusters (using, e.g. K-means clustering). This turns our time series into a $t$ dimensional vector, where each entry is the index of that timestep's assigned cluster.\n",
    "\n",
    "Then, we create a transfer matrix from this series, for a given choice of the transition time scale, $\\tau$. \n",
    "We create a $C\\times C$ dimensional matrix whose $ij$-th component is the likelihood of moving from cluster $i$ to cluster $j$ in $\\tau$ timesteps. \n",
    "\n",
    "Note: the downsampling rate and transfer matrix transition time scale have roughly the same function. It is, however, computationally easier to downsample as much as possible first.\n",
    "\n",
    "\n",
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# NETWORK PARAMETERS\n",
    "# ------------------------\n",
    "\n",
    "# Main parameters:\n",
    "COUPLING_STRENGTH = 1.1 # Chosen to be just above the chaotic transition (allowing for finite-size effects)\n",
    "N_NEURONS = 100 # Chosen to be low enough to be easy to work with, yet high enough for a clean Lyapunov spectrum\n",
    "\n",
    "# The following parameters influence temporal resolution and trajectory duration\n",
    "TIMESTEP = 0.1 # Short enough that the lyapunov spectrum is still clean\n",
    "N_STEPS = 500000\n",
    "N_BURNIN_STEPS = 500\n",
    "\n",
    "# ------------------------\n",
    "# COARSE-GRAINING IN SPACE\n",
    "# ------------------------\n",
    "\n",
    "# We can either set the explicit choice of pca dimension or determine it from the lyapunov spectrum\n",
    "PCA_DIMENSION = \"lyapunov\" #  int | \"lyapunov\"\n",
    "\n",
    "# Explicitly coarse-graining the phase space by clustering (should be << # of downsampled timesteps)\n",
    "N_CLUSTERS = 100 # This is balanced against the transition timescale. If one is higher, the other can be lower.\n",
    "\n",
    "# -----------------------\n",
    "# COARSE-GRAINING IN TIME \n",
    "# ------------------------\n",
    "\n",
    "# Increasing downsampling and state_duration have about the same effect: \n",
    "# it increases the length of the time-scales we are interested in\n",
    "DOWNSAMPLE_RATE = 500\n",
    "TRANSITION_TIMESCALE = 1\n",
    "\n",
    "# -----------------------\n",
    "# STATE-SPACE RECONSTRUCTION\n",
    "# ------------------------\n",
    " \n",
    "# N delays tells me how many times to stack states into my definition of state.\n",
    "N_DELAYS = 0\n",
    "\n",
    "# Embedding dimension is the downprojected space after doing a delay embedding\n",
    "EMBEDDING_DIM = 3\n",
    "\n",
    "# -----------------------\n",
    "# PLOTTING PARAMETERS\n",
    "# ------------------------\n",
    "\n",
    "EIGENVALUE_RANGE = [1, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding finite size effects\n",
    "\n",
    "For finite-size networks, the transition to chaos is spread. Instead of the network becoming spontaneously chaotic as you increase the coupling strength through the transition, it goes through a number of stages:\n",
    "1. First, the trivial zero-activity fixed point splits into non-zero fixed points.\n",
    "2. Next, the non-zero fixed points becoming oscillatory cycles.\n",
    "3. Finally, the cycles becoming chaotic. Exactly how is not yet clear.\n",
    "\n",
    "### Maximum Lyapunov exponent scaling with coupling strength\n",
    "The first way to parse this transition is to look at the value of the maximum lyapunov exponent as you increase the coupling strength (for a fixed network size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUPLING_STRENGTHS = [0.5, 0.8, 0.9, 1.0, 1.1, 1.2, 1.5, 2.0, 3.0]\n",
    "\n",
    "#plot_max_l_with_g(gs=COUPLING_STRENGTHS, n_dofs = N_NEURONS, timestep=TIMESTEP, n_steps=N_STEPS, n_burn_in=N_BURNIN_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic phase diagram\n",
    "\n",
    "Though the above makes it clear where chaos really begins (up from where it would be in the infinite-network limit), it does not tell us anything about the intermediate phases (i.e. non-zero fixed points and limit cycles).\n",
    "    \n",
    "#### Fixed point at zero. \n",
    "\n",
    "The first additional sign we can get is by considering what fraction of the neurons settle to the fixed point, where we consider a neuron to be at the trivial fixed point if all the points in a given neuron's history (after a suitable burn-in) are within a desired tolerance of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOL = 1e-3\n",
    "\n",
    "#plot_trivial_fixed_pt_with_g(gs=COUPLING_STRENGTHS, n_dofs = N_NEURONS, timestep=TIMESTEP, n_steps=N_STEPS, n_burn_in=N_BURNIN_STEPS, atol=ATOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-trivial fixed points\n",
    "\n",
    "The next thing we can do is to study the fraction of neurons that settle to a non-zero fixed point, by the same criterion of \"settling\" as used for the trivial point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_nontrivial_fixed_pt_with_g(gs=COUPLING_STRENGTHS, n_dofs = N_NEURONS, timestep=TIMESTEP, n_steps=N_STEPS, n_burn_in=N_BURNIN_STEPS, atol=ATOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycles\n",
    "\n",
    "It starts to get harder when you go looking for cycles. An obvious first-order approach to uncovering cycles is to look for places where the autocorrelation goes through $1$. If such points exist, then the activity is cyclical with the corresponding timescale as its period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Deriving fraction at 0 for `g = 0.5`\n",
      "INFO:root:Loading from save ./saves/trajectories/trajectory-8c6c6b8d1732859ab41514012bdffb1e.npy\n",
      "Counting cycles...:   1%|          | 1/100 [01:09<1:55:11, 69.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7df005e4830c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_cycles_with_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOUPLING_STRENGTHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dofs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_NEURONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIMESTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_burn_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_BURNIN_STEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mATOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/masters-thesis/code/randnn/randnn/plotting.py\u001b[0m in \u001b[0;36mplot_cycles_with_g\u001b[0;34m(gs, n_dofs, timestep, n_steps, n_burn_in, t_ons, atol)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0mtrajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcont_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_burn_in\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_burn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mn_cycles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_cycles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0mcycle_proportions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_cycles\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_dofs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/masters-thesis/code/randnn/randnn/utils.py\u001b[0m in \u001b[0;36mcount_cycles\u001b[0;34m(trajectory, atol)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mpath_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mpath_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_normalized\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0macor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_normalized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"full\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpath_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0macor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Autocorrelation is symmetrical about the half-way point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# TODO: Use a more efficient way\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcorrelate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/randnn-PaBUC3Au/lib/python3.8/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mcorrelate\u001b[0;34m(a, v, mode)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \"\"\"\n\u001b[1;32m    712\u001b[0m     \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_mode_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_N_STEPS = 50000\n",
    "\n",
    "plot_cycles_with_g(gs=COUPLING_STRENGTHS, n_dofs = N_NEURONS, timestep=TIMESTEP, n_steps=N_STEPS, n_burn_in=N_BURNIN_STEPS, atol=ATOL, max_n_steps=MAX_N_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_randnn_spectrum(coupling_strength=COUPLING_STRENGTH):\n",
    "    @downsample(DOWNSAMPLE_RATE)\n",
    "    def get_pca_reduced_trajectory(dimension=PCA_DIMENSION, coupling_strength=coupling_strength, n_dofs=N_NEURONS, n_steps=N_STEPS, n_burn_in=N_BURNIN_STEPS, t_ons=100):\n",
    "        cont_nn = ContinuousNN(coupling_strength=coupling_strength, n_dofs=n_dofs, max_step=TIMESTEP)\n",
    "        trajectory = cont_nn.run(n_steps=n_steps, n_burn_in=n_burn_in)\n",
    "        lyapunov_spectrum = cont_nn.get_lyapunov_spectrum(trajectory, t_ons=t_ons)\n",
    "\n",
    "        if dimension == \"lyapunov\":\n",
    "            dimension = int(np.ceil(get_attractor_dimension(lyapunov_spectrum)))\n",
    "\n",
    "        pca = PCA(dimension)\n",
    "        return pca.fit_transform(trajectory)\n",
    "\n",
    "    trajectory = get_pca_reduced_trajectory()\n",
    "    transfer_operator = TransferOperator(labeling_method=\"kmeans\", n_delays=N_DELAYS, embedding_dim=EMBEDDING_DIM)\n",
    "\n",
    "    def get_trans_matrix(time_series=None, n_clusters=N_CLUSTERS, n_future_timesteps=TRANSITION_TIMESCALE):\n",
    "        return transfer_operator.get_trans_matrix(time_series=time_series, n_clusters=n_clusters, n_future_timesteps=n_future_timesteps)\n",
    "\n",
    "    trans_matrix = get_trans_matrix(time_series=trajectory)\n",
    "    spectrum, _ = eigsort(trans_matrix.T, 100, which='LR')\n",
    "\n",
    "    plot_eig_spectrum(spectrum, EIGENVALUE_RANGE, \"Eigenvalue spectrum of the transfer matrix.\", label=\"Coupling strength, $g = {}$\".format(coupling_strength))\n",
    "    \n",
    "plot_randnn_spectrum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_randnn_spectra(gs=[.9, 1., 1.1, 1.5, 2., 3, 5]):\n",
    "    plt.title(\"Eigenvalue spectra for different $g$\")\n",
    "    for g in gs:\n",
    "        plot_randnn_spectrum(g)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_randnn_spectra()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_t_imp_scaling(time_series, eigval_idx, n_clusters_list=[50, 100, 200, 500, 1000], transition_timescales=range(1, 20, 2), timestep=1):\n",
    "    plt.title(\"Scaling of $t$ with $\\\\tau$ and $n_p$\")\n",
    "    for n_clusters in n_clusters_list:\n",
    "        t_imps = np.zeros(len(transition_timescales))\n",
    "        for i, tau in tqdm(enumerate(transition_timescales), desc=\"computing t_imp\"):\n",
    "            t_imps[i] = transfer_operator.get_t_imp(time_series, [eigval_idx], n_clusters, tau, timestep)[0]\n",
    "            \n",
    "        plt.plot(transition_timescales, t_imps, label=\"$n_p = {}$\".format(n_clusters))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_t_imp_scaling(trajectory[:100000:10], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scaling](./photos/randnn-t-imp-scaling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_t_imps(time_series, eigval_idxs=range(1,3), n_clusters=N_CLUSTERS, transition_timescales=range(1, 10, 2), timestep=1):\n",
    "    plt.title(\"Scaling of $t$ with $\\\\tau$ and $n_p$\")\n",
    "    t_imps = np.zeros((len(transition_timescales), len(eigval_idxs)))\n",
    "\n",
    "    for i, tau in tqdm(enumerate(transition_timescales), desc=\"computing t_imp\"):\n",
    "        t_imps[i, :] = transfer_operator.get_t_imp(time_series, eigval_idxs, n_clusters, tau, timestep)\n",
    "    \n",
    "    for i in eigval_idxs:\n",
    "        plt.plot(transition_timescales, t_imps[:, i])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_t_imps(trajectory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
